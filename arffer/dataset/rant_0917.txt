[RANT] Why is it that this society only wants things to end against women?!

This is somewhat of a gentle speech, not really a rant, but I think this sub is fitting enough. 

For example: Breast cancer.

Now, imagine they are doing a jog for the cure around the hospital your are staying in for possibly terminal brain cancer. Now imagine them, chanting to cure it. They aren't focusing on you and others with this. You feel left out, knowing you have a rare cancer, nobody* knows you're here. Your family is wrapped up in the jog. Nobody knows you're suffering and going to die because nobody is reaching out. Yeah.

Imagine that feeling.

Just end cancer altogether! Make October CANCER awareness month.

Why end violence against women? End violence altogether!

Why end rape against women? End rape altogether! AND QUIT FUCKING IMPLYING THAT ONLY MEN RAPE, MEDIA.

Why end discrimination against women? End discrimination altogether!

Why fight for equal women's rights? Fight for all equal rights!

Why end gender roles against women? Fight for all gender roles to be removed!

And fucking disband the selective fucking sexist service. It's sexist implying that all men are willing to get themselves killed, mentally or physically scarred for a gigantic land claim.

Do you see what I'm saying? I hope I got my point across. Things happen to men to! Equal rights for all! Not just women, or black people, everyone deserves good fortune.